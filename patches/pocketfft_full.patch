# *******************************************************************************
# Copyright 2021 Arm Limited and affiliates.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# *******************************************************************************
# This patch file is composed of numerous commits from
# the PyTorch repository available at:
# https://github.com/pytorch/pytorch
#
# The commits aggregated in this patch are the following:
# 278494d774d529098d341d7da5e236a869e019c6
# cc06fc83edb272361705ae8f14cd81fd47f11fa9
# 3b22dc4be3b4e46d0ea296adbfe74efe1bff5512
# 651ed68794a7cf06027daec1f74a4ab51dd08b48
# 94b89e36979dbafcff73c8b2fcb987bedf295f3f
# 6e5d065b2b7c6e98d0757b49122f10990bac4ba9
#
# which is work done in the following pull requests:
# https://github.com/pytorch/pytorch/pull/60976
# https://github.com/pytorch/pytorch/pull/62841
#
# In addition, minor manual changes have been added
# in BUILD.bazel and cmake/Modules/FindMKL.cmake
# to automate preparation for a PocketFFT enabled
# build.
# *******************************************************************************
diff --git a/BUILD.bazel b/BUILD.bazel
index 00408a7f79..458ea7b14d 100644
--- a/BUILD.bazel
+++ b/BUILD.bazel
@@ -536,6 +536,7 @@ header_template_rule(
         "@AT_MKLDNN_ENABLED@": "1",
         "@AT_MKL_ENABLED@": "0",
         "@AT_FFTW_ENABLED@": "0",
+        "@AT_POCKETFFT_ENABLED@": "1",
         "@AT_NNPACK_ENABLED@": "0",
         "@CAFFE2_STATIC_LINK_CUDA_INT@": "0",
         "@USE_BLAS@": "1",
diff --git a/aten/src/ATen/Config.h.in b/aten/src/ATen/Config.h.in
index 38326491be..e315f3ce64 100644
--- a/aten/src/ATen/Config.h.in
+++ b/aten/src/ATen/Config.h.in
@@ -9,6 +9,7 @@
 #define AT_MKLDNN_ENABLED() @AT_MKLDNN_ENABLED@
 #define AT_MKL_ENABLED() @AT_MKL_ENABLED@
 #define AT_FFTW_ENABLED() @AT_FFTW_ENABLED@
+#define AT_POCKETFFT_ENABLED() @AT_POCKETFFT_ENABLED@
 #define AT_NNPACK_ENABLED() @AT_NNPACK_ENABLED@
 #define CAFFE2_STATIC_LINK_CUDA() @CAFFE2_STATIC_LINK_CUDA_INT@
 #define AT_BUILD_WITH_BLAS() @USE_BLAS@
diff --git a/aten/src/ATen/native/mkl/SpectralOps.cpp b/aten/src/ATen/native/mkl/SpectralOps.cpp
index a79c8a3675..de8e0752c8 100644
--- a/aten/src/ATen/native/mkl/SpectralOps.cpp
+++ b/aten/src/ATen/native/mkl/SpectralOps.cpp
@@ -5,66 +5,10 @@
 #include <ATen/NativeFunctions.h>
 #include <c10/util/accumulate.h>
 
-#if !AT_MKL_ENABLED()
-
-namespace at { namespace native {
-
-// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
-REGISTER_NO_CPU_DISPATCH(fft_fill_with_conjugate_symmetry_stub, fft_fill_with_conjugate_symmetry_fn);
-
-Tensor _fft_c2r_mkl(const Tensor& self, IntArrayRef dim, int64_t normalization, int64_t last_dim_size) {
-  AT_ERROR("fft: ATen not compiled with MKL support");
-}
-
-Tensor _fft_r2c_mkl(const Tensor& self, IntArrayRef dim, int64_t normalization, bool onesided) {
-  AT_ERROR("fft: ATen not compiled with MKL support");
-}
-
-Tensor _fft_c2c_mkl(const Tensor& self, IntArrayRef dim, int64_t normalization, bool forward) {
-  AT_ERROR("fft: ATen not compiled with MKL support");
-}
-
-Tensor& _fft_r2c_mkl_out(const Tensor& self, IntArrayRef dim, int64_t normalization,
-                         bool onesided, Tensor& out) {
-  AT_ERROR("fft: ATen not compiled with MKL support");
-}
-
-Tensor& _fft_c2r_mkl_out(const Tensor& self, IntArrayRef dim, int64_t normalization,
-                         int64_t last_dim_size, Tensor& out) {
-  AT_ERROR("fft: ATen not compiled with MKL support");
-}
-
-Tensor& _fft_c2c_mkl_out(const Tensor& self, IntArrayRef dim, int64_t normalization,
-                         bool forward, Tensor& out) {
-  AT_ERROR("fft: ATen not compiled with MKL support");
-}
-
-}}
-
-#else // AT_MKL_ENABLED
-
-#include <ATen/ATen.h>
-#include <ATen/Config.h>
-#include <ATen/Dispatch.h>
-#include <ATen/NativeFunctions.h>
+#if AT_MKL_ENABLED() || AT_POCKETFFT_ENABLED()
 #include <ATen/Parallel.h>
-#include <ATen/Utils.h>
-
-#include <ATen/native/TensorIterator.h>
-
-#include <algorithm>
-#include <vector>
-#include <numeric>
-#include <cmath>
-
-#include <mkl_dfti.h>
-#include <ATen/mkl/Exceptions.h>
-#include <ATen/mkl/Descriptors.h>
-#include <ATen/mkl/Limits.h>
-
 
 namespace at { namespace native {
-
 // In real-to-complex transform, MKL FFT only fills half of the values due to
 // conjugate symmetry. See native/SpectralUtils.h for more details.
 // The following structs are used to fill in the other half with symmetry in
@@ -207,6 +151,186 @@ REGISTER_ARCH_DISPATCH(fft_fill_with_conjugate_symmetry_stub, DEFAULT, &_fft_fil
 REGISTER_AVX_DISPATCH(fft_fill_with_conjugate_symmetry_stub, &_fft_fill_with_conjugate_symmetry_cpu_)
 REGISTER_AVX2_DISPATCH(fft_fill_with_conjugate_symmetry_stub, &_fft_fill_with_conjugate_symmetry_cpu_)
 
+// _out variants can be shared between PocketFFT and MKL
+Tensor& _fft_r2c_mkl_out(const Tensor& self, IntArrayRef dim, int64_t normalization,
+                         bool onesided, Tensor& out) {
+  auto result = _fft_r2c_mkl(self, dim, normalization, /*onesided=*/true);
+  if (onesided) {
+    resize_output(out, result.sizes());
+    return out.copy_(result);
+  }
+
+  resize_output(out, self.sizes());
+
+  auto last_dim = dim.back();
+  auto last_dim_halfsize = result.sizes()[last_dim];
+  auto out_slice = out.slice(last_dim, 0, last_dim_halfsize);
+  out_slice.copy_(result);
+  at::native::_fft_fill_with_conjugate_symmetry_(out, dim);
+  return out;
+}
+
+Tensor& _fft_c2r_mkl_out(const Tensor& self, IntArrayRef dim, int64_t normalization,
+                         int64_t last_dim_size, Tensor& out) {
+  auto result = _fft_c2r_mkl(self, dim, normalization, last_dim_size);
+  resize_output(out, result.sizes());
+  return out.copy_(result);
+}
+
+Tensor& _fft_c2c_mkl_out(const Tensor& self, IntArrayRef dim, int64_t normalization,
+                         bool forward, Tensor& out) {
+  auto result = _fft_c2c_mkl(self, dim, normalization, forward);
+  resize_output(out, result.sizes());
+  return out.copy_(result);
+}
+
+}} // namespace at::native
+#endif /* AT_MKL_ENALED() || AT_POCKETFFT_ENABLED() */
+
+#if AT_POCKETFFT_ENABLED()
+#include <pocketfft_hdronly.h>
+
+namespace at { namespace native {
+
+namespace {
+using namespace pocketfft;
+
+stride_t stride_from_tensor(const Tensor& t) {
+  stride_t stride(t.strides().begin(), t.strides().end());
+  for(auto& s: stride) {
+   s *= t.element_size();
+  }
+  return stride;
+}
+
+inline shape_t shape_from_tensor(const Tensor& t) {
+  return shape_t(t.sizes().begin(), t.sizes().end());
+}
+
+template<typename T>
+inline std::complex<T> *tensor_cdata(Tensor& t) {
+  return reinterpret_cast<std::complex<T>*>(t.data<c10::complex<T>>());
+}
+
+template<typename T>
+inline const std::complex<T> *tensor_cdata(const Tensor& t) {
+  return reinterpret_cast<const std::complex<T>*>(t.data<c10::complex<T>>());
+}
+
+template<typename T>
+T compute_fct(int64_t size, int64_t normalization) {
+  constexpr auto one = static_cast<T>(1);
+  switch (static_cast<fft_norm_mode>(normalization)) {
+    case fft_norm_mode::none: return one;
+    case fft_norm_mode::by_n: return one / static_cast<T>(size);
+    case fft_norm_mode::by_root_n: return one / std::sqrt(static_cast<T>(size));
+  }
+  AT_ERROR("Unsupported normalization type", normalization);
+}
+
+template<typename T>
+T compute_fct(const Tensor& t, IntArrayRef dim, int64_t normalization) {
+  if (static_cast<fft_norm_mode>(normalization) == fft_norm_mode::none) {
+    return static_cast<T>(1);
+  }
+  const auto& sizes = t.sizes();
+  int64_t n = 1;
+  for(auto idx: dim) {
+    n *= sizes[idx];
+  }
+  return compute_fct<T>(n, normalization);
+}
+
+} // anonymous namespace
+
+Tensor _fft_c2r_mkl(const Tensor& self, IntArrayRef dim, int64_t normalization, int64_t last_dim_size) {
+  auto in_sizes = self.sizes();
+  DimVector out_sizes(in_sizes.begin(), in_sizes.end());
+  out_sizes[dim.back()] = last_dim_size;
+  auto out = at::empty(out_sizes, self.options().dtype(c10::toValueType(self.scalar_type())));
+  pocketfft::shape_t axes(dim.begin(), dim.end());
+  if (self.scalar_type() == kComplexFloat) {
+    pocketfft::c2r(shape_from_tensor(out), stride_from_tensor(self), stride_from_tensor(out), axes, false,
+                   tensor_cdata<float>(self),
+                   out.data<float>(), compute_fct<float>(out, dim, normalization));
+  } else {
+    pocketfft::c2r(shape_from_tensor(out), stride_from_tensor(self), stride_from_tensor(out), axes, false,
+                   tensor_cdata<double>(self),
+                   out.data<double>(), compute_fct<double>(out, dim, normalization));
+    }
+  return out;
+}
+
+
+Tensor _fft_r2c_mkl(const Tensor& self, IntArrayRef dim, int64_t normalization, bool onesided) {
+  TORCH_CHECK(self.is_floating_point());
+  auto input_sizes = self.sizes();
+  DimVector out_sizes(input_sizes.begin(), input_sizes.end());
+  auto last_dim = dim.back();
+  auto last_dim_halfsize = (input_sizes[last_dim]) / 2 + 1;
+  if (onesided) {
+    out_sizes[last_dim] = last_dim_halfsize;
+  }
+
+  auto out = at::empty(out_sizes, self.options().dtype(c10::toComplexType(self.scalar_type())));
+  pocketfft::shape_t axes(dim.begin(), dim.end());
+  if (self.scalar_type() == kFloat) {
+    pocketfft::r2c(shape_from_tensor(self), stride_from_tensor(self), stride_from_tensor(out), axes, true,
+                   self.data<float>(),
+                   tensor_cdata<float>(out), compute_fct<float>(self, dim, normalization));
+  } else {
+    pocketfft::r2c(shape_from_tensor(self), stride_from_tensor(self), stride_from_tensor(out), axes, true,
+                   self.data<double>(),
+                   tensor_cdata<double>(out), compute_fct<double>(self, dim, normalization));
+  }
+
+  if (!onesided) {
+    at::native::_fft_fill_with_conjugate_symmetry_(out, dim);
+  }
+  return out;
+}
+
+Tensor _fft_c2c_mkl(const Tensor& self, IntArrayRef dim, int64_t normalization, bool forward) {
+  TORCH_CHECK(self.is_complex());
+  auto out = at::empty(self.sizes(), self.options());
+  pocketfft::shape_t axes(dim.begin(), dim.end());
+  if (self.scalar_type() == kComplexFloat) {
+    pocketfft::c2c(shape_from_tensor(self), stride_from_tensor(self), stride_from_tensor(out), axes, forward,
+                   tensor_cdata<float>(self),
+                   tensor_cdata<float>(out), compute_fct<float>(self, dim, normalization));
+  } else {
+    pocketfft::c2c(shape_from_tensor(self), stride_from_tensor(self), stride_from_tensor(out), axes, forward,
+                   tensor_cdata<double>(self),
+                   tensor_cdata<double>(out), compute_fct<double>(self, dim, normalization));
+  }
+
+  return out;
+}
+
+}}
+
+#elif AT_MKL_ENABLED()
+#include <ATen/ATen.h>
+#include <ATen/Config.h>
+#include <ATen/Dispatch.h>
+#include <ATen/NativeFunctions.h>
+#include <ATen/Utils.h>
+
+#include <ATen/native/TensorIterator.h>
+
+#include <algorithm>
+#include <vector>
+#include <numeric>
+#include <cmath>
+
+#include <mkl_dfti.h>
+#include <ATen/mkl/Exceptions.h>
+#include <ATen/mkl/Descriptors.h>
+#include <ATen/mkl/Limits.h>
+
+
+namespace at { namespace native {
+
 // Constructs an mkl-fft plan descriptor representing the desired transform
 // For complex types, strides are in units of 2 * element_size(dtype)
 // sizes are for the full signal, including batch size and always two-sided
@@ -400,13 +524,6 @@ Tensor _fft_c2r_mkl(const Tensor& self, IntArrayRef dim, int64_t normalization,
   return _exec_fft(out, input, out_sizes, dim, normalization, /*forward=*/false);
 }
 
-Tensor& _fft_c2r_mkl_out(const Tensor& self, IntArrayRef dim, int64_t normalization,
-                         int64_t last_dim_size, Tensor& out) {
-  auto result = _fft_c2r_mkl(self, dim, normalization, last_dim_size);
-  resize_output(out, result.sizes());
-  return out.copy_(result);
-}
-
 // n-dimensional real to complex FFT
 Tensor _fft_r2c_mkl(const Tensor& self, IntArrayRef dim, int64_t normalization, bool onesided) {
   TORCH_CHECK(self.is_floating_point());
@@ -428,24 +545,6 @@ Tensor _fft_r2c_mkl(const Tensor& self, IntArrayRef dim, int64_t normalization,
   return out;
 }
 
-Tensor& _fft_r2c_mkl_out(const Tensor& self, IntArrayRef dim, int64_t normalization,
-                         bool onesided, Tensor& out) {
-  auto result = _fft_r2c_mkl(self, dim, normalization, /*onesided=*/true);
-  if (onesided) {
-    resize_output(out, result.sizes());
-    return out.copy_(result);
-  }
-
-  resize_output(out, self.sizes());
-
-  auto last_dim = dim.back();
-  auto last_dim_halfsize = result.sizes()[last_dim];
-  auto out_slice = out.slice(last_dim, 0, last_dim_halfsize);
-  out_slice.copy_(result);
-  at::native::_fft_fill_with_conjugate_symmetry_(out, dim);
-  return out;
-}
-
 // n-dimensional complex to complex FFT/IFFT
 Tensor _fft_c2c_mkl(const Tensor& self, IntArrayRef dim, int64_t normalization, bool forward) {
   TORCH_CHECK(self.is_complex());
@@ -454,13 +553,40 @@ Tensor _fft_c2c_mkl(const Tensor& self, IntArrayRef dim, int64_t normalization,
   return _exec_fft(out, self, self.sizes(), sorted_dims, normalization, forward);
 }
 
+}} // namespace at::native
+
+#else
+
+namespace at { namespace native {
+// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
+REGISTER_NO_CPU_DISPATCH(fft_fill_with_conjugate_symmetry_stub, fft_fill_with_conjugate_symmetry_fn);
+
+Tensor _fft_c2r_mkl(const Tensor& self, IntArrayRef dim, int64_t normalization, int64_t last_dim_size) {
+  AT_ERROR("fft: ATen not compiled with FFT support");
+}
+
+Tensor _fft_r2c_mkl(const Tensor& self, IntArrayRef dim, int64_t normalization, bool onesided) {
+  AT_ERROR("fft: ATen not compiled with FFT support");
+}
+
+Tensor _fft_c2c_mkl(const Tensor& self, IntArrayRef dim, int64_t normalization, bool forward) {
+  AT_ERROR("fft: ATen not compiled with FFT support");
+}
+
+Tensor& _fft_r2c_mkl_out(const Tensor& self, IntArrayRef dim, int64_t normalization,
+                         bool onesided, Tensor& out) {
+  AT_ERROR("fft: ATen not compiled with FFT support");
+}
+
+Tensor& _fft_c2r_mkl_out(const Tensor& self, IntArrayRef dim, int64_t normalization,
+                         int64_t last_dim_size, Tensor& out) {
+  AT_ERROR("fft: ATen not compiled with FFT support");
+}
+
 Tensor& _fft_c2c_mkl_out(const Tensor& self, IntArrayRef dim, int64_t normalization,
                          bool forward, Tensor& out) {
-  auto result = _fft_c2c_mkl(self, dim, normalization, forward);
-  resize_output(out, result.sizes());
-  return out.copy_(result);
+  AT_ERROR("fft: ATen not compiled with FFT support");
 }
 
 }} // namespace at::native
-
 #endif
diff --git a/caffe2/CMakeLists.txt b/caffe2/CMakeLists.txt
index 50ebb224ce..00eda617b5 100644
--- a/caffe2/CMakeLists.txt
+++ b/caffe2/CMakeLists.txt
@@ -565,6 +565,11 @@ if(NOT INTERN_BUILD_MOBILE OR NOT BUILD_CAFFE2_MOBILE)
     set_source_files_properties(${TORCH_SRC_DIR}/csrc/jit/tensorexpr/llvm_codegen.cpp PROPERTIES COMPILE_FLAGS -Wno-init-list-lifetime)
   endif()
 
+  # Pass path to PocketFFT
+  if(AT_POCKETFFT_ENABLED)
+    set_source_files_properties(${CMAKE_CURRENT_LIST_DIR}/../aten/src/ATen/native/mkl/SpectralOps.cpp PROPERTIES INCLUDE_DIRECTORIES "${POCKETFFT_INCLUDE_DIR}")
+  endif()
+
   if(NOT INTERN_DISABLE_MOBILE_INTERP)
     set(MOBILE_SRCS
        ${TORCH_SRC_DIR}/csrc/jit/mobile/function.cpp
diff --git a/cmake/Dependencies.cmake b/cmake/Dependencies.cmake
index 5d57b9ca78..382731a23d 100644
--- a/cmake/Dependencies.cmake
+++ b/cmake/Dependencies.cmake
@@ -216,6 +216,19 @@ if(USE_FFTW OR NOT MKL_FOUND)
   endif()
 endif()
 
+# --- [ PocketFFT
+set(AT_POCKETFFT_ENABLED 0)
+if(NOT MKL_FOUND)
+  find_path(POCKETFFT_INCLUDE_DIR NAMES pocketfft_hdronly.h PATHS
+            /usr/local/include
+            "$ENV{POCKETFFT_HOME}"
+            "${PROJECT_SOURCE_DIR}/third_party/pocketfft"
+           )
+  if(POCKETFFT_INCLUDE_DIR AND CMAKE_VERSION VERSION_GREATER "3.9")
+    set(AT_POCKETFFT_ENABLED 1)
+  endif()
+endif()
+
 # ---[ Dependencies
 # NNPACK and family (QNNPACK, PYTORCH_QNNPACK, and XNNPACK) can download and
 # compile their dependencies in isolation as part of their build.  These dependencies
diff --git a/cmake/Modules/FindMKL.cmake b/cmake/Modules/FindMKL.cmake
index b79a874662..48881360ad 100644
--- a/cmake/Modules/FindMKL.cmake
+++ b/cmake/Modules/FindMKL.cmake
@@ -378,20 +378,22 @@ ENDIF (MKL_LIBRARIES)
 # Final
 SET(CMAKE_LIBRARY_PATH ${saved_CMAKE_LIBRARY_PATH})
 SET(CMAKE_INCLUDE_PATH ${saved_CMAKE_INCLUDE_PATH})
-IF (MKL_LIBRARIES AND MKL_INCLUDE_DIR)
-  SET(MKL_FOUND TRUE)
-ELSE (MKL_LIBRARIES AND MKL_INCLUDE_DIR)
-  if (MKL_LIBRARIES AND NOT MKL_INCLUDE_DIR)
-    MESSAGE(WARNING "MKL libraries files are found, but MKL header files are \
-      not. You can get them by `conda install mkl-include` if using conda (if \
-      it is missing, run `conda upgrade -n root conda` first), and \
-      `pip install mkl-devel` if using pip. If build fails with header files \
-      available in the system, please make sure that CMake will search the \
-      directory containing them, e.g., by setting CMAKE_INCLUDE_PATH.")
-  endif()
-  SET(MKL_FOUND FALSE)
-  SET(MKL_VERSION)  # clear MKL_VERSION
-ENDIF (MKL_LIBRARIES AND MKL_INCLUDE_DIR)
+#IF (MKL_LIBRARIES AND MKL_INCLUDE_DIR)
+#  SET(MKL_FOUND TRUE)
+#ELSE (MKL_LIBRARIES AND MKL_INCLUDE_DIR)
+#  if (MKL_LIBRARIES AND NOT MKL_INCLUDE_DIR)
+#    MESSAGE(WARNING "MKL libraries files are found, but MKL header files are \
+#      not. You can get them by `conda install mkl-include` if using conda (if \
+#      it is missing, run `conda upgrade -n root conda` first), and \
+#      `pip install mkl-devel` if using pip. If build fails with header files \
+#      available in the system, please make sure that CMake will search the \
+#      directory containing them, e.g., by setting CMAKE_INCLUDE_PATH.")
+#  endif()
+#  SET(MKL_FOUND FALSE)
+#  SET(MKL_VERSION)  # clear MKL_VERSION
+#ENDIF (MKL_LIBRARIES AND MKL_INCLUDE_DIR)
+SET(MKL_FOUND FALSE)
+SET(MKL_VERSION)  # clear MKL_VERSION
 
 # Standard termination
 IF(NOT MKL_FOUND AND MKL_FIND_REQUIRED)
diff --git a/setup.py b/setup.py
index 06b96e0b5d..1edf5b5ac5 100644
--- a/setup.py
+++ b/setup.py
@@ -339,7 +339,7 @@ def check_submodules():
             print('Please run:\n\tgit submodule update --init --recursive')
             sys.exit(1)
     for folder in folders:
-        check_for_files(folder, ["CMakeLists.txt", "Makefile", "setup.py", "LICENSE"])
+        check_for_files(folder, ["CMakeLists.txt", "Makefile", "setup.py", "LICENSE", "LICENSE.md", "LICENSE.txt"])
     check_for_files(os.path.join(third_party_path, 'fbgemm', 'third_party',
                                  'asmjit'), ['CMakeLists.txt'])
     check_for_files(os.path.join(third_party_path, 'onnx', 'third_party',
diff --git a/test/test_spectral_ops.py b/test/test_spectral_ops.py
index e71d9800bb..a2a96c9f64 100644
--- a/test/test_spectral_ops.py
+++ b/test/test_spectral_ops.py
@@ -9,9 +9,8 @@ from torch.testing._internal.common_utils import \
     (TestCase, run_tests, TEST_NUMPY, TEST_LIBROSA, TEST_MKL)
 from torch.testing._internal.common_device_type import \
     (instantiate_device_type_tests, ops, dtypes, onlyOnCPUAndCUDA,
-     skipCPUIfNoMkl, skipCUDAIfRocm, deviceCountAtLeast, onlyCUDA, OpDTypes,
-     skipIf)
-from torch.testing._internal.common_methods_invocations import spectral_funcs
+     skipCPUIfNoFFT, deviceCountAtLeast, onlyCUDA, OpDTypes, skipIf)
+from torch.testing._internal.common_methods_invocations import spectral_funcs, SpectralFuncInfo
 
 from setuptools import distutils
 from typing import Optional, List
@@ -136,8 +135,7 @@ class TestFFT(TestCase):
             actual = op(input, *args)
             self.assertEqual(actual, expected, exact_dtype=exact_dtype)
 
-    @skipCUDAIfRocm
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     @onlyOnCPUAndCUDA
     @dtypes(torch.float, torch.double, torch.complex64, torch.complex128)
     def test_fft_round_trip(self, device, dtype):
@@ -198,7 +196,7 @@ class TestFFT(TestCase):
         with self.assertRaisesRegex(RuntimeError, "ihfft expects a real input tensor"):
             torch.fft.ihfft(t)
 
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     @onlyOnCPUAndCUDA
     @dtypes(torch.int8, torch.float, torch.double, torch.complex64, torch.complex128)
     def test_fft_type_promotion(self, device, dtype):
@@ -277,8 +275,7 @@ class TestFFT(TestCase):
                 actual = op(input, s, dim, norm)
                 self.assertEqual(actual, expected, exact_dtype=exact_dtype)
 
-    @skipCUDAIfRocm
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     @onlyOnCPUAndCUDA
     @dtypes(torch.float, torch.double, torch.complex64, torch.complex128)
     def test_fftn_round_trip(self, device, dtype):
@@ -343,8 +340,7 @@ class TestFFT(TestCase):
     # NOTE: 2d transforms are only thin wrappers over n-dim transforms,
     # so don't require exhaustive testing.
 
-    @skipCPUIfNoMkl
-    @skipCUDAIfRocm
+    @skipCPUIfNoFFT
     @onlyOnCPUAndCUDA
     @dtypes(torch.double, torch.complex128)
     def test_fft2_numpy(self, device, dtype):
@@ -387,8 +383,7 @@ class TestFFT(TestCase):
                     actual = fn(input, s, dim, norm)
                     self.assertEqual(actual, expected)
 
-    @skipCUDAIfRocm
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     @onlyOnCPUAndCUDA
     @dtypes(torch.float, torch.complex64)
     def test_fft2_fftn_equivalence(self, device, dtype):
@@ -425,7 +420,7 @@ class TestFFT(TestCase):
 
                 self.assertEqual(actual, expect)
 
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     @onlyOnCPUAndCUDA
     def test_fft2_invalid(self, device):
         a = torch.rand(10, 10, 10, device=device)
@@ -451,7 +446,7 @@ class TestFFT(TestCase):
 
     # Helper functions
 
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     @onlyOnCPUAndCUDA
     @unittest.skipIf(not TEST_NUMPY, 'NumPy not found')
     @dtypes(torch.float, torch.double)
@@ -477,7 +472,7 @@ class TestFFT(TestCase):
                 actual = torch_fn(*args, device=device, dtype=dtype)
                 self.assertEqual(actual, expected, exact_dtype=False)
 
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     @onlyOnCPUAndCUDA
     @dtypes(torch.float, torch.double)
     def test_fftfreq_out(self, device, dtype):
@@ -489,7 +484,7 @@ class TestFFT(TestCase):
             self.assertEqual(actual, expect)
 
 
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     @onlyOnCPUAndCUDA
     @unittest.skipIf(not TEST_NUMPY, 'NumPy not found')
     @dtypes(torch.float, torch.double, torch.complex64, torch.complex128)
@@ -515,7 +510,7 @@ class TestFFT(TestCase):
                 actual = torch_fn(input, dim=dim)
                 self.assertEqual(actual, expected)
 
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     @onlyOnCPUAndCUDA
     @unittest.skipIf(not TEST_NUMPY, 'NumPy not found')
     @dtypes(torch.float, torch.double)
@@ -594,8 +589,7 @@ class TestFFT(TestCase):
         _test_complex((40, 60, 3, 80), 3, lambda x: x.transpose(2, 0).select(0, 2)[5:55, :, 10:])
         _test_complex((30, 55, 50, 22), 3, lambda x: x[:, 3:53, 15:40, 1:21])
 
-    @skipCUDAIfRocm
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     @onlyOnCPUAndCUDA
     @dtypes(torch.double)
     def test_fft_ifft_rfft_irfft(self, device, dtype):
@@ -675,7 +669,7 @@ class TestFFT(TestCase):
                         self.assertEqual(torch.backends.cuda.cufft_plan_cache.max_size, 11)  # default is cuda:1
 
     # passes on ROCm w/ python 2.7, fails w/ python 3.6
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     @onlyOnCPUAndCUDA
     @dtypes(torch.double)
     def test_stft(self, device, dtype):
@@ -744,7 +738,7 @@ class TestFFT(TestCase):
 
     @skipCUDAIfRocm
     @onlyOnCPUAndCUDA
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     @dtypes(torch.double, torch.cdouble)
     def test_complex_stft_roundtrip(self, device, dtype):
         test_args = list(product(
@@ -787,7 +781,7 @@ class TestFFT(TestCase):
 
     @skipCUDAIfRocm
     @onlyOnCPUAndCUDA
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     @dtypes(torch.double, torch.cdouble)
     def test_stft_roundtrip_complex_window(self, device, dtype):
         test_args = list(product(
@@ -827,8 +821,7 @@ class TestFFT(TestCase):
                 self.assertEqual(x_roundtrip, x)
 
 
-    @skipCUDAIfRocm
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     @dtypes(torch.cdouble)
     def test_complex_stft_definition(self, device, dtype):
         test_args = list(product(
@@ -849,7 +842,7 @@ class TestFFT(TestCase):
 
     @skipCUDAIfRocm
     @onlyOnCPUAndCUDA
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     @dtypes(torch.cdouble)
     def test_complex_stft_real_equiv(self, device, dtype):
         test_args = list(product(
@@ -882,8 +875,7 @@ class TestFFT(TestCase):
                                 center=center, normalized=normalized)
             self.assertEqual(expected, actual)
 
-    @skipCUDAIfRocm
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     @dtypes(torch.cdouble)
     def test_complex_istft_real_equiv(self, device, dtype):
         test_args = list(product(
@@ -909,7 +901,7 @@ class TestFFT(TestCase):
                                  return_complex=True)
             self.assertEqual(expected, actual)
 
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     def test_complex_stft_onesided(self, device):
         # stft of complex input cannot be onesided
         for x_dtype, window_dtype in product((torch.double, torch.cdouble), repeat=2):
@@ -931,14 +923,14 @@ class TestFFT(TestCase):
 
     # stft is currently warning that it requires return-complex while an upgrader is written
     @onlyOnCPUAndCUDA
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     def test_stft_requires_complex(self, device):
         x = torch.rand(100)
         y = x.stft(10, pad_mode='constant')
         # with self.assertRaisesRegex(RuntimeError, 'stft requires the return_complex parameter'):
         #     y = x.stft(10, pad_mode='constant')
 
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     def test_fft_input_modification(self, device):
         # FFT functions should not modify their input (gh-34551)
 
@@ -960,7 +952,7 @@ class TestFFT(TestCase):
 
     @skipCUDAIfRocm
     @onlyOnCPUAndCUDA
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     @dtypes(torch.double)
     def test_istft_round_trip_simple_cases(self, device, dtype):
         """stft -> istft should recover the original signale"""
@@ -974,7 +966,7 @@ class TestFFT(TestCase):
 
     @skipCUDAIfRocm
     @onlyOnCPUAndCUDA
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     @dtypes(torch.double)
     def test_istft_round_trip_various_params(self, device, dtype):
         """stft -> istft should recover the original signale"""
@@ -1077,7 +1069,7 @@ class TestFFT(TestCase):
 
     @skipCUDAIfRocm
     @onlyOnCPUAndCUDA
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     @dtypes(torch.double)
     def test_istft_of_sine(self, device, dtype):
         def _test(amplitude, L, n):
@@ -1112,7 +1104,7 @@ class TestFFT(TestCase):
 
     @skipCUDAIfRocm
     @onlyOnCPUAndCUDA
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     @dtypes(torch.double)
     def test_istft_linearity(self, device, dtype):
         num_trials = 100
@@ -1179,7 +1171,7 @@ class TestFFT(TestCase):
 
     @skipCUDAIfRocm
     @onlyOnCPUAndCUDA
-    @skipCPUIfNoMkl
+    @skipCPUIfNoFFT
     def test_batch_istft(self, device):
         original = torch.tensor([
             [[4., 0.], [4., 0.], [4., 0.], [4., 0.], [4., 0.]],
@@ -1219,6 +1211,53 @@ class TestFFT(TestCase):
             torch.istft(x.to(device), n_fft=100, window=window)
 
 
+class FFTDocTestFinder:
+    '''The default doctest finder doesn't like that function.__module__ doesn't
+    match torch.fft. It assumes the functions are leaked imports.
+    '''
+    def __init__(self):
+        self.parser = doctest.DocTestParser()
+
+    def find(self, obj, name=None, module=None, globs=None, extraglobs=None):
+        doctests = []
+
+        modname = name if name is not None else obj.__name__
+        globs = dict() if globs is None else globs
+
+        for fname in obj.__all__:
+            func = getattr(obj, fname)
+            if inspect.isroutine(func):
+                qualname = modname + '.' + fname
+                docstring = inspect.getdoc(func)
+                if docstring is None:
+                    continue
+
+                examples = self.parser.get_doctest(
+                    docstring, globs=globs, name=fname, filename=None, lineno=None)
+                doctests.append(examples)
+
+        return doctests
+
+
+class TestFFTDocExamples(TestCase):
+    pass
+
+def generate_doc_test(doc_test):
+    def test(self, device):
+        self.assertEqual(device, 'cpu')
+        runner = doctest.DocTestRunner()
+        runner.run(doc_test)
+
+        if runner.failures != 0:
+            runner.summarize()
+            self.fail('Doctest failed')
+
+    setattr(TestFFTDocExamples, 'test_' + doc_test.name, skipCPUIfNoFFT(test))
+
+for doc_test in FFTDocTestFinder().find(torch.fft, globs=dict(torch=torch)):
+    generate_doc_test(doc_test)
+
+
 instantiate_device_type_tests(TestFFT, globals())
 
 if __name__ == '__main__':
diff --git a/torch/_C/__init__.pyi.in b/torch/_C/__init__.pyi.in
index cf4f8a35f0..f6978f1ef5 100644
--- a/torch/_C/__init__.pyi.in
+++ b/torch/_C/__init__.pyi.in
@@ -601,6 +601,7 @@ has_lapack: _bool
 has_cuda: _bool
 has_mkldnn: _bool
 has_cudnn: _bool
+has_spectral: _bool
 _GLIBCXX_USE_CXX11_ABI: _bool
 default_generator: Generator
 
diff --git a/torch/csrc/Module.cpp b/torch/csrc/Module.cpp
index cd934bd0ab..cbc7df938b 100644
--- a/torch/csrc/Module.cpp
+++ b/torch/csrc/Module.cpp
@@ -927,6 +927,13 @@ PyObject* initModule() {
 #endif
  ASSERT_TRUE(set_module_attr("has_cudnn", has_cudnn));
 
+#if AT_MKL_ENABLED() || AT_POCKETFFT_ENABLED()
+  PyObject *has_spectral = Py_True;
+#else
+  PyObject *has_spectral = Py_False;
+#endif
+ ASSERT_TRUE(set_module_attr("has_spectral", has_spectral));
+
   // force ATen to initialize because it handles
   // setting up TH Errors so that they throw C++ exceptions
   at::init();
diff --git a/torch/testing/_internal/common_device_type.py b/torch/testing/_internal/common_device_type.py
index 5427cb4551..dd87f11085 100644
--- a/torch/testing/_internal/common_device_type.py
+++ b/torch/testing/_internal/common_device_type.py
@@ -916,6 +916,11 @@ def skipCPUIfNoLapack(fn):
     return skipCPUIf(not torch._C.has_lapack, "PyTorch compiled without Lapack")(fn)
 
 
+# Skips a test on CPU if FFT is not available.
+def skipCPUIfNoFFT(fn):
+    return skipCPUIf(not torch._C.has_spectral, "PyTorch is built without FFT support")(fn)
+
+
 # Skips a test on CPU if MKL is not available.
 def skipCPUIfNoMkl(fn):
     return skipCPUIf(not TEST_MKL, "PyTorch is built without MKL support")(fn)
diff --git a/torch/testing/_internal/common_methods_invocations.py b/torch/testing/_internal/common_methods_invocations.py
index fd45a76efa..42ec5a0e0f 100644
--- a/torch/testing/_internal/common_methods_invocations.py
+++ b/torch/testing/_internal/common_methods_invocations.py
@@ -21,7 +21,7 @@ from torch.testing import \
 from .._core import _dispatch_dtypes
 from torch.testing._internal.common_device_type import \
     (skipIf, skipCUDAIfNoMagma, skipCUDAIfNoMagmaAndNoCusolver, skipCUDAIfNoCusolver,
-     skipCPUIfNoLapack, skipCPUIfNoMkl, skipCUDAIfRocm, precisionOverride,)
+     skipCPUIfNoLapack, skipCPUIfNoFFT, skipCUDAIfRocm, precisionOverride, toleranceOverride, tol)
 from torch.testing._internal.common_cuda import CUDA11OrLater, SM53OrLater
 from torch.testing._internal.common_utils import \
     (is_iterable_of_tensors,
@@ -2008,7 +2008,7 @@ class SpectralFuncInfo(OpInfo):
                  **kwargs):
         decorators = list(decorators) if decorators is not None else []
         decorators += [
-            skipCPUIfNoMkl,
+            skipCPUIfNoFFT,
             skipCUDAIfRocm,
             # gradgrad is quite slow
             DecorateInfo(slowTest, 'TestGradients', 'test_fn_gradgrad'),
